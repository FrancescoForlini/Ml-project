{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Web Scraping"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "773e8e3c4f69bffe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65e0699d1e3b76de"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-01T20:28:20.524186Z",
     "start_time": "2024-05-01T20:28:20.519069Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove tuples without hrefs (with None instead in all rows)\n",
    "\n",
    "def remove_tuples(df):  \n",
    "    n_df = df.copy()\n",
    "    for attr in n_df.columns:\n",
    "        # if (isinstance(n_df[attr][0], tuple)) and all(n_df[attr][c][1] is None for c in n_df[attr]):\n",
    "        # if all(isinstance(n_df[attr][i], tuple) for i in range(len(n_df))) and all(d[1] is None for d in n_df[attr]):\n",
    "        if all(isinstance(d1, tuple) for d1 in n_df[attr]) and all(d2[1] is None for d2 in n_df[attr]):\n",
    "            n_df[attr] = [c[0] for c in n_df[attr]]\n",
    "    return n_df # right"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# \"numbers\" in effective numbers (float). N.B.: Wt can be empty\n",
    "\n",
    "def int_numbers(df):  # only not tuple object (they don't have href) may be numbers (data is only numbers)\n",
    "    n_df = df.copy()\n",
    "    for attr in n_df.columns:\n",
    "        if not any(isinstance(d, tuple) or any(c.isalpha() for c in d) or '-' in d for d in n_df[attr]):\n",
    "            try:\n",
    "                n_df[attr] = n_df[attr].astype(float)\n",
    "            except ValueError:\n",
    "                print(\"It's Wt for sure so\" )\n",
    "                n_df.loc[n_df['Wt'] == '', 'Wt'] = '0'\n",
    "                n_df['Wt'] = n_df['Wt'].astype(float)\n",
    "                continue\n",
    "    return n_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T20:28:21.613911Z",
     "start_time": "2024-05-01T20:28:21.608876Z"
    }
   },
   "id": "f31e8a08d1a758a6",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# \"numbers\" in effective numbers (float). N.B.: Wt can be empty\n",
    "\n",
    "def int_numbers1(df):  # only not tuple object (they don't have href) may be numbers\n",
    "    n_df = df.copy()\n",
    "    for k, series in enumerate([n_df[attr] for attr in n_df.columns]):\n",
    "        if not any(isinstance(s, tuple) for s in series):\n",
    "            series = series.astype(float)\n",
    "            print(series)\n",
    "    return n_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T20:28:22.118990Z",
     "start_time": "2024-05-01T20:28:22.115127Z"
    }
   },
   "id": "ba31e292fec7a290",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Transformation table\n",
    "\n",
    "def zoom_table(df, r):\n",
    "    n_df = df.copy()\n",
    "    # career_index = n_df.index[n_df['Season'] == 'Career']\n",
    "    n_df = n_df.loc[n_df['Season'] == 'Career']\n",
    "    n_df = n_df.drop(columns=['Season', 'Tm', 'Lg'])\n",
    "    n_df['From'] = r['From'].astype(int)\n",
    "    n_df['To'] = r['To'].astype(int)\n",
    "    n_df['Hof'] = [True if t[0][-1] == '*' else False for t in r['Player']]\n",
    "    n_df['Player'] = r['Player'][0].replace('*', '')\n",
    "    \n",
    "    return n_df\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T20:28:22.873650Z",
     "start_time": "2024-05-01T20:28:22.868782Z"
    }
   },
   "id": "9fed18c34c0eeda2",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_none_advanced_dataframe(row1):\n",
    "    return pd.DataFrame({\n",
    "        'Age': None, 'Pos': None, 'G': None, 'MP': None, 'PER': None, 'TS%': None, '3PAr': None, 'FTr': None,\n",
    "        'ORB%': None, 'DRB%': None, 'TRB%': None, 'AST%': None, 'STL%': None, 'BLK%': None, 'TOV%': None, 'USG%': None,\n",
    "        'Unnamed: 19': None, 'OWS': None, 'DWS': None, 'WS': None, 'WS/48': None, 'Boh2': None, 'OBPM': None,\n",
    "        'DBPM': None, 'BPM': None, 'VORP': None, 'From': row1['From'].astype(int), 'To': row1['To'].astype(int),\n",
    "        'Hof': False, 'Player': row1['Player'], 'Unnamed: 16': None, 'Unnamed: 21': None, 'Unnamed: 18': None,\n",
    "        'Unnamed: 23': None\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T20:28:23.729564Z",
     "start_time": "2024-05-01T20:28:23.725133Z"
    }
   },
   "id": "9f3f8900d5bd64b4",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Taking statistics like per 100 possession statistics and avdanced statistics\n",
    "\n",
    "def per_100_and_advanced_statistics(absolute_path, p_table, driver1):\n",
    "    df_per_100 = []\n",
    "    df_ad = []\n",
    "    \n",
    "    for index, row in p_table.iterrows():  # for each plyer\n",
    "        player, href = row['Player']\n",
    "        print(f\"Let's go with player {player} from {row['From']}\")\n",
    "        driver1.get(absolute_path + href)\n",
    "\n",
    "        ################# Per_100_poss #################\n",
    "        try: \n",
    "            dfp_t = driver1.find_element(By.XPATH, '//table[@id=\"per_poss\"]')\n",
    "        except NoSuchElementException: \n",
    "            dfp_t = driver1.find_element(By.XPATH, '//table[@id=\"per_minute\"]')  #  or \"per_game\"\n",
    "        dfp = pd.read_html(StringIO(dfp_t.get_attribute('outerHTML')))[0]\n",
    "        #  print(df1)  #\n",
    "        dfp = zoom_table(dfp, row)\n",
    "        df_per_100.append(dfp)\n",
    "        \n",
    "        ################# Advanced #################\n",
    "        try:\n",
    "            dfa_t = driver1.find_element(By.XPATH, '//table[@id=\"advanced\"]')\n",
    "            dfa = pd.read_html(StringIO(dfa_t.get_attribute('outerHTML')))[0]\n",
    "\n",
    "            dfa = zoom_table(dfa, row)\n",
    "        except NoSuchElementException:\n",
    "            dfa = create_none_advanced_dataframe(row)\n",
    "        df_ad.append(dfa)\n",
    "        \n",
    "        \n",
    "        time.sleep(7)  # otherwise I'll be kicked\n",
    "    return df_per_100, df_ad"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T20:28:24.221086Z",
     "start_time": "2024-05-01T20:28:24.215544Z"
    }
   },
   "id": "5896e08d6d2bd40d",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# One letter at time\n",
    "\n",
    "dir_path = \".\\\\Datasets\"\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "bf_players_path = \"https://www.basketball-reference.com/players/\"\n",
    "bf_absolute_path = \"https://www.basketball-reference.com\"\n",
    "list_p_100_p = []\n",
    "list_a = []\n",
    "driver = webdriver.Firefox()\n",
    "letter = None\n",
    "\n",
    "for letter in alphabet[25:26]:  # z\n",
    "    letter_path = bf_players_path + letter\n",
    "    driver.get(letter_path)\n",
    "    players_character_table = pd.read_html(letter_path, extract_links='body')[0]\n",
    "\n",
    "    players_l_table = remove_tuples(players_character_table)\n",
    "    players_l_table = int_numbers(players_l_table)\n",
    "    size_b = players_l_table.shape[0]\n",
    "    players_l_table = players_l_table.loc[\n",
    "        (players_l_table['From'] >= 1973) & (players_l_table['From'] < 2021)].reset_index(drop=True)\n",
    "    size_a = players_l_table.shape[0]\n",
    "    print(\"%d players got eliminated: \" % (size_b - size_a))\n",
    "    df_p_100_p, df_a = per_100_and_advanced_statistics(bf_absolute_path, players_l_table, driver)\n",
    "    [list_p_100_p.append(d) for d in df_p_100_p]  # list for each letter\n",
    "    # dataset_p_100_p.append(df_p_100_p)\n",
    "    [list_a.append(d) for d in df_a]  # list for each letter\n",
    "    #dataset_a.append(df_a)\n",
    "\n",
    "driver.close()\n",
    "print(letter)\n",
    "\n",
    "################# Per_100_poss #################\n",
    "dataset_per_100_poss = pd.concat(list_p_100_p, ignore_index=True)\n",
    "# dataset_per_100_poss.to_csv(dir_path + \"\\\\Per_100_poss_csv\\\\\" + letter + \"_p_100_p.csv\")\n",
    "# dataset_per_100_poss.to_csv(dir_path + \"\\\\Per_100_poss_excel\\\\\" + letter + \"_p_100_p.xlsx\")\n",
    "dataset_per_100_poss.to_json(dir_path + \"\\\\Per_100_poss_json\\\\\" + letter + \"_p_100_p.json\", orient='records', indent=3)\n",
    "\n",
    "################# Advanced #################\n",
    "dataset_advanced = pd.concat(list_a, ignore_index=True)\n",
    "# dataset_advanced.to_csv(dir_path + \"\\\\Advanced_csv\\\\\" + letter + \"_advanced.csv\")\n",
    "# dataset_advanced.to_csv(dir_path + \"\\\\Advanced_excel\\\\\" + letter + \"_advanced.xlsx\")\n",
    "dataset_advanced.to_json(dir_path + \"\\\\Advanced_json\\\\\" + letter + \"_advanced.json\", orient='records', indent=3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d66a8cc79fa2e09",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Join single letter files\n",
    "\n",
    "dir_path = \".\\\\Datasets\"\n",
    "\n",
    "################# Per_100_poss #################\n",
    "folder_path_per_100_poss = \".\\\\Per_100_poss_json\"\n",
    "per_100_poss_list = []\n",
    "for filename in os.listdir(folder_path_per_100_poss):\n",
    "    file_path = os.path.join(folder_path_per_100_poss, filename)\n",
    "    df_f = pd.read_json(file_path, orient='records')\n",
    "    per_100_poss_list.append(df_f)\n",
    "df_all_per_100_poss_json = pd.concat(per_100_poss_list, ignore_index=True)\n",
    "# df_all_per_100_poss_json.to_csv(dir_path + \"\\\\training_per_100_poss_players.csv\")\n",
    "# df_all_per_100_poss_json.to_excel(dir_path + \"\\\\training_per_100_poss_players.xlsx\")\n",
    "df_all_per_100_poss_json.to_json(dir_path + \"\\\\training_per_100_poss_players.json\", orient='records', indent=3)\n",
    "\n",
    "################# Advanced #################\n",
    "folder_path_advanced = dir_path + \"\\\\Advanced_json\"\n",
    "advanced_list = []\n",
    "for filename in os.listdir(folder_path_advanced):\n",
    "    file_path = os.path.join(folder_path_advanced, filename)\n",
    "    df_f = pd.read_json(file_path, orient='records')\n",
    "    advanced_list.append(df_f)\n",
    "df_all_advanced_json = pd.concat(advanced_list, ignore_index=True)\n",
    "# df_all_per_100_poss_json.to_csv(dir_path + \"\\\\training_advanced_players.csv\")\n",
    "# df_all_per_100_poss_json.to_excel(dir_path + \"\\\\training_advanced_players.xlsx\")\n",
    "df_all_advanced_json.to_json(dir_path + \"\\\\training_advanced_players.json\", orient='records', indent=3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce392077d221c125"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# All together\n",
    "\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "dir_path = \".\\\\Datasets\"\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "bf_players_path = \"https://www.basketball-reference.com/players/\"\n",
    "bf_absolute_path = \"https://www.basketball-reference.com\"\n",
    "list_p_100_p = []\n",
    "list_a = []\n",
    "driver = webdriver.Firefox()\n",
    "letter = None\n",
    "\n",
    "for letter in alphabet:  # a-z\n",
    "    letter_path = bf_players_path + letter\n",
    "    driver.get(letter_path)\n",
    "    players_character_table = pd.read_html(letter_path, extract_links='body')[0]\n",
    "\n",
    "    players_l_table = remove_tuples(players_character_table)  # list(players_character_table.columns) anche senza list\n",
    "    players_l_table = int_numbers(players_l_table)\n",
    "    size_b = players_l_table.shape[0]\n",
    "    players_l_table = players_l_table.loc[\n",
    "        (players_l_table['From'] >= 1973) & (players_l_table['From'] < 2021)].reset_index(drop=True)\n",
    "    size_a = players_l_table.shape[0]\n",
    "    print(\"%d players got eliminated: \" % (size_b - size_a))\n",
    "    df_p_100_p, df_a = per_100_and_advanced_statistics(bf_absolute_path, players_l_table, driver)\n",
    "    [list_p_100_p.append(d) for d in df_p_100_p]  # list for each letter\n",
    "    # dataset_p_100_p.append(df_p_100_p)\n",
    "    [list_a.append(d) for d in df_a]  # list for each letter\n",
    "    #dataset_a.append(df_a)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "################# Per_100_poss #################\n",
    "dataset_per_100_poss = pd.concat(list_p_100_p, ignore_index=True)\n",
    "# dataset_per_100_poss.to_csv(dir_path + \"training_per_100_poss_players.csv\")\n",
    "# dataset_per_100_poss.to_excel(dir_path + \"training_per_100_poss_players.xlsx\")\n",
    "dataset_per_100_poss.to_json(dir_path + \"\\\\training_per_100_poss_players.json\", orient='records', indent=3)\n",
    "\n",
    "################# Advanced #################\n",
    "dataset_advanced = pd.concat(list_a, ignore_index=True)\n",
    "# dataset_advanced.to_csv(dir_path + \"\\\\training_advanced_players.csv\")\n",
    "# dataset_advanced.to_excel(dir_path + \"\\\\training_advanced_players.xlsx\")\n",
    "dataset_advanced.to_json(dir_path + \"\\\\training_advanced_players.json\", orient='records', indent=3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "990b6d3e972b7ca",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training and test set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad42d36d651606c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Difference between sets (df1 - df2) \n",
    "\n",
    "def df_difference1(df1, df2):\n",
    "    merged = pd.merge(df1, df2, on='nome', how='left', indicator=True)\n",
    "    df_filtered = merged[merged['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "    df_not_filtered = merged[merged['_merge'] != 'left_only'].drop(columns='_merge')\n",
    "    \n",
    "    return df_filtered, df_not_filtered"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44807d51e1bb6464"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Difference between sets (df1 - df2) \n",
    "\n",
    "def df_difference(df1, df2):\n",
    "    df_s1, df_s2 = set(df1['Player']), set(df2['Player'])\n",
    "    df_diff = df_s1 - df_s2\n",
    "\n",
    "    return df1[df1['Player'].isin(list(df_diff))], df1[df1['Player'].isin(list(df_s2))]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "484231fa6a28e71d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def making_test_datasets(path_file, test_path, hof_cand_pl):\n",
    "    pl_df = pd.read_json(path_file, orient='records')\n",
    "    pl_df, hof_cand_pl = df_difference1(pl_df, hof_cand_pl)\n",
    "    pl_df.to_json(path_file, orient='records', indent=3)\n",
    "\n",
    "    hof_cand_players_df = pd.merge(pl_df, hof_cand_pl, how='left', on='Player') # \n",
    "    hof_cand_players_df['Hof'] = [True if float(d) > 0.5 else False for d in hof_cand_players_df['HoF Prob']]\n",
    "    hof_cand_players_df.to_json(test_path, orient='records', indent=3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7c9d533f313c86"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Dropping candidate hof players into another json to make dataset test. Hof_candidates (hof_candidate_advanced,\n",
    "# hof_candidates_per_100_poss) with hof prob. > 0.5 are 'effectively' hof\n",
    "\n",
    "dir_path = \".\\\\Datasets\"\n",
    "\n",
    "hof_test_players_path = \"https://www.basketball-reference.com/leaders/hof_prob.html\"\n",
    "hof_candidate_players = pd.read_html(hof_test_players_path)[1]\n",
    "hof_candidate_players = hof_candidate_players.drop(columns=['Rank'])\n",
    "\n",
    "################# Per_100_poss #################\n",
    "all_per_100_poss_file_path = dir_path + \"\\\\training_per_100_poss_players.json\"\n",
    "hof_per_100_poss_file_path = dir_path + \"\\\\test_per_100_poss_players.json\"\n",
    "making_test_datasets(all_per_100_poss_file_path, hof_per_100_poss_file_path, hof_candidate_players)\n",
    "\n",
    "################# Advanced #################\n",
    "all_advanced_file_path = dir_path + \"\\\\training_advanced_players.json\"\n",
    "hof_advanced_file_path = dir_path + \"\\\\test_advanced_players.json\"\n",
    "making_test_datasets(all_advanced_file_path, hof_advanced_file_path, hof_candidate_players)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acfef17102b17ce7",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
